{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec5f956",
   "metadata": {},
   "source": [
    "# Take the following code to and run it on Google Colab use GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b85177",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pyngrok nest_asyncio fastapi uvicorn\n",
    "\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "port = 7860\n",
    "\n",
    "ngrok.kill()\n",
    "ngrok.set_auth_token(\"Your Private Key from NGROK\")\n",
    "\n",
    "public_url = ngrok.connect(port, bind_tls=True)\n",
    "print(f\"ðŸ”— Public FastAPI URL: {public_url}/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84748980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Form, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import uuid, os, torch, traceback, asyncio\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from typing import Optional\n",
    "import base64\n",
    "from io import BytesIO\n",
    "# ========== setting FastAPI ==========\n",
    "app = FastAPI()\n",
    "\n",
    "OUTPUT_DIR = \"stable_diffusion_output\"\n",
    "EDITED_DIR = \"edited_images\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(EDITED_DIR, exist_ok=True)\n",
    "\n",
    "app.mount(\"/generated\", StaticFiles(directory=OUTPUT_DIR), name=\"generated\")\n",
    "app.mount(\"/edited\", StaticFiles(directory=EDITED_DIR), name=\"edited\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========== Load Models ==========\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(device)\n",
    "pipe2 = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# ========== Concurrency Control ==========\n",
    "pipe_semaphore = asyncio.Semaphore(2)  # allow 2 concurrent generations\n",
    "\n",
    "# ØªØ­Ø³ÙŠÙ† Ø¥Ø¶Ø§ÙÙŠ: Ø§Ø³ØªØ®Ø¯Ø§Ù… xformers Ø¥Ù† ØªÙˆÙØ±\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    pipe2.enable_xformers_memory_efficient_attention()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ========== ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ²Ø§Ù…Ù† ==========\n",
    "pipe_semaphore = asyncio.Semaphore(2)  # ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø¯ Ø­Ø³Ø¨ Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø²\n",
    "\n",
    "# ========== Ø§Ù„Ù‡ÙŠØ§ÙƒÙ„ ==========\n",
    "class Prompt(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class EditImageRequest(BaseModel):\n",
    "    prompt: str\n",
    "    image_base64: str\n",
    "\n",
    "# ========== ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±) ==========\n",
    "@app.post(\"/generate_image\")\n",
    "async def generate_image_endpoint(prompt: Prompt):\n",
    "    try:\n",
    "        async with pipe_semaphore:\n",
    "            pipe.scheduler.set_timesteps(30)\n",
    "            result = pipe(prompt.text, num_inference_steps=30)\n",
    "            image = result.images[0]\n",
    "\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_bytes = buffered.getvalue()\n",
    "        img_base64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "        return {\"image_base64\": img_base64}\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
    "\n",
    "# ========== ØªÙˆÙ„ÙŠØ¯ Ø¹Ø¯Ø© ØµÙˆØ± Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© ==========\n",
    "async def generate_image_batch_async(prompt_text: str, count: int = 4) -> list:\n",
    "    async with pipe_semaphore:\n",
    "        try:\n",
    "            pipe.scheduler.set_timesteps(30)\n",
    "            result = pipe(prompt_text, num_inference_steps=30, num_images_per_prompt=count)\n",
    "\n",
    "            urls = []\n",
    "            for image in result.images:\n",
    "                filename = f\"{uuid.uuid4().hex}.png\"\n",
    "                file_path = os.path.join(OUTPUT_DIR, filename)\n",
    "                image.save(file_path)\n",
    "                urls.append(f\"generated/{filename}\")\n",
    "\n",
    "            return urls\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return [\"error: exception\"]\n",
    "\n",
    "@app.post(\"/multi_image_generation\")\n",
    "async def multi_image_generation(prompt: Prompt, count: int = 4):\n",
    "    try:\n",
    "        if not prompt.text.strip():\n",
    "            return JSONResponse(content={\"error\": \"Prompt is required.\"}, status_code=400)\n",
    "\n",
    "        urls = await generate_image_batch_async(prompt.text, count)\n",
    "        return {\"urls\": urls}\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
    "\n",
    "# ========== ØªØ¹Ø¯ÙŠÙ„ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… img2img ==========\n",
    "@app.post(\"/edit_image\")\n",
    "async def edit_image(request: EditImageRequest):\n",
    "    try:\n",
    "        image_data = base64.b64decode(request.image_base64)\n",
    "        img = Image.open(BytesIO(image_data)).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "        async with pipe_semaphore:\n",
    "            pipe2.scheduler.set_timesteps(50)\n",
    "            result = pipe2(\n",
    "                prompt=request.prompt,\n",
    "                image=img,\n",
    "                strength=0.75,\n",
    "                guidance_scale=7.5,\n",
    "                num_inference_steps=50,\n",
    "            )\n",
    "\n",
    "        edited_image = result.images[0]\n",
    "\n",
    "        buffered = BytesIO()\n",
    "        edited_image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "        return {\"image_base64\": img_base64}\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
    "\n",
    "@app.post(\"/is_available\")\n",
    "async def check():\n",
    "    return {\"ready\": True}\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… ==========\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irym_0.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
